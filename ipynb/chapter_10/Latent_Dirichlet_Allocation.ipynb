{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "- 토픽 모델링 방법 중 하나\n",
    "    - 토픽 모델링 : <br> 각각의 텍스트(문장 혹은 문단)은 특정 주제(정보의 카테고리)의 집합이라는 가정하에 각 텍스트가 포함하는 주제를 통계적 방법(주제에 관련된 단어의 등장 빈도수 등)을 이용해 파악하는 방법. <br> 여러가지 방법이 있는데, 그 중 NMF(비음수 행렬인수분해)를 이용해 특징(단어)를 함축적인 차원으로 압축해 주제를 특정해내는 방법 등이 있다.\n",
    "- 주제별 단어 분포수를 바탕으로, 주어진 텍스트의 단어수 분포를 분석해서 해당 텍스트가 어떤 주제를 다루고 있는지 예측한다.\n",
    "- TF-IDF를 필두로 하여 잠재 의미 분석(Latent semantic indexing, LSI), 확률 잠재 의미 분석(Probabilistic latent semantic analysis, pLSA)을 거쳐서 고안된 방법 (디리클레 분포를 따른다)\n",
    "- (주관적인 의견이지만) '잠재'라는 이름이 붙은 이유는 클러스터링을 한 뒤 각 센트로이드가 정확히 무엇이다라고 분류할 순 없지만 \"잠재적으로 각 차원의 특징을 반영하는 어떤 의미를 지닐 것이다.\"라고 가정하듯이, 각 단어를 이용해 어떤 잠재적인 의미(주제)를 찾아내는 기법이라 그런 것 같다. (NMF를 생각하면 이해가 빠를 듯)\n",
    "- 그러나, bag of words가 그렇듯이 단어의 순서 정보는 반영하지 못한다. 즉, 단어의 교환성을 가정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load input data\n",
    "def load_data(input_file) : \n",
    "    data = []\n",
    "    with open(input_file, \"r\") as fp :\n",
    "        for line in fp.readlines() : \n",
    "            data.append(line[:-1])\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize, remove stopwords, and extract word stem\n",
    "def process(input_text) : \n",
    "    #create object\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    stemmer = SnowballStemmer(\"english\") \n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    tokens = tokenizer.tokenize(input_text.lower()) \n",
    "    tokens = [x for x in tokens if not x in stop_words]\n",
    "    tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "    \n",
    "    return tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Roman empire expanded very rapidly and it was the biggest empire in the world for a long time.',\n",
       " 'An algebraic structure is a set with one or more finitary operations defined on it that satisfies a list of axioms.',\n",
       " 'Renaissance started as a cultural movement in Italy in the Late Medieval period and later spread to the rest of Europe.',\n",
       " 'The line of demarcation between prehistoric and historical times is crossed when people cease to live only in the present.',\n",
       " 'Mathematicians seek out patterns and use them to formulate new conjectures.  ',\n",
       " 'A notational symbol that represents a number is called a numeral in mathematics. ',\n",
       " 'The process of extracting the underlying essence of a mathematical concept is called abstraction.',\n",
       " 'Historically, people have frequently waged wars against each other in order to expand their empires.',\n",
       " 'Ancient history indicates that various outside influences have helped formulate the culture and traditions of Eastern Europe.',\n",
       " 'Mappings between sets which preserve structures are of special interest in many fields of mathematics.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [process(x) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['roman',\n",
       "  'empir',\n",
       "  'expand',\n",
       "  'rapid',\n",
       "  'biggest',\n",
       "  'empir',\n",
       "  'world',\n",
       "  'long',\n",
       "  'time'],\n",
       " ['algebra',\n",
       "  'structur',\n",
       "  'set',\n",
       "  'one',\n",
       "  'finitari',\n",
       "  'oper',\n",
       "  'defin',\n",
       "  'satisfi',\n",
       "  'list',\n",
       "  'axiom'],\n",
       " ['renaiss',\n",
       "  'start',\n",
       "  'cultur',\n",
       "  'movement',\n",
       "  'itali',\n",
       "  'late',\n",
       "  'mediev',\n",
       "  'period',\n",
       "  'later',\n",
       "  'spread',\n",
       "  'rest',\n",
       "  'europ'],\n",
       " ['line',\n",
       "  'demarc',\n",
       "  'prehistor',\n",
       "  'histor',\n",
       "  'time',\n",
       "  'cross',\n",
       "  'peopl',\n",
       "  'ceas',\n",
       "  'live',\n",
       "  'present'],\n",
       " ['mathematician', 'seek', 'pattern', 'use', 'formul', 'new', 'conjectur'],\n",
       " ['notat', 'symbol', 'repres', 'number', 'call', 'numer', 'mathemat'],\n",
       " ['process',\n",
       "  'extract',\n",
       "  'under',\n",
       "  'essenc',\n",
       "  'mathemat',\n",
       "  'concept',\n",
       "  'call',\n",
       "  'abstract'],\n",
       " ['histor', 'peopl', 'frequent', 'wage', 'war', 'order', 'expand', 'empir'],\n",
       " ['ancient',\n",
       "  'histori',\n",
       "  'indic',\n",
       "  'various',\n",
       "  'outsid',\n",
       "  'influenc',\n",
       "  'help',\n",
       "  'formul',\n",
       "  'cultur',\n",
       "  'tradit',\n",
       "  'eastern',\n",
       "  'europ'],\n",
       " ['map',\n",
       "  'set',\n",
       "  'preserv',\n",
       "  'structur',\n",
       "  'special',\n",
       "  'interest',\n",
       "  'mani',\n",
       "  'field',\n",
       "  'mathemat']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tokens = corpora.Dictionary(tokens)\n",
    "doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1)],\n",
       " [(18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1)],\n",
       " [(6, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1)],\n",
       " [(39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)],\n",
       " [(46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1)],\n",
       " [(46, 1), (47, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1)],\n",
       " [(1, 1), (2, 1), (33, 1), (36, 1), (59, 1), (60, 1), (61, 1), (62, 1)],\n",
       " [(18, 1),\n",
       "  (19, 1),\n",
       "  (40, 1),\n",
       "  (63, 1),\n",
       "  (64, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 1),\n",
       "  (70, 1),\n",
       "  (71, 1)],\n",
       " [(16, 1),\n",
       "  (17, 1),\n",
       "  (47, 1),\n",
       "  (72, 1),\n",
       "  (73, 1),\n",
       "  (74, 1),\n",
       "  (75, 1),\n",
       "  (76, 1),\n",
       "  (77, 1)]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 2 #argument for LDA model\n",
    "ldamodel = models.ldamodel.LdaModel(doc_term_mat, num_topics=num_topics, id2word=dict_tokens, passes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 5 #essential words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"cultur\" + 0.022*\"europ\" + 0.022*\"formul\" + 0.022*\"call\" + 0.022*\"time\"'),\n",
       " (1,\n",
       "  '0.027*\"expand\" + 0.027*\"structur\" + 0.027*\"set\" + 0.027*\"empir\" + 0.027*\"histor\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=num_topics, num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 contributing words to each topic:\n",
      "\n",
      "Topic 0\n",
      "\"cultur\" ==> 2.2%\n",
      "\"europ\" ==> 2.2%\n",
      "\"formul\" ==> 2.2%\n",
      "\"call\" ==> 2.2%\n",
      "\"time\" ==> 2.2%\n",
      "\n",
      "Topic 1\n",
      "\"expand\" ==> 2.7%\n",
      "\"structur\" ==> 2.7%\n",
      "\"set\" ==> 2.7%\n",
      "\"empir\" ==> 2.7%\n",
      "\"histor\" ==> 2.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop \" + str(num_words) + \" contributing words to each topic:\")\n",
    "for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words) : \n",
    "    print(\"\\nTopic\", item[0])\n",
    "    \n",
    "    #essential words\n",
    "    list_of_strings = item[1].split(\" + \")\n",
    "    for text in list_of_strings : \n",
    "        weight = text.split(\"*\")[0]\n",
    "        word = text.split(\"*\")[1]\n",
    "        print(word, \"==>\", str(round(float(weight)*100, 2)) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
