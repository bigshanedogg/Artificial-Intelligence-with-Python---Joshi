{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"all\") #용량이 3기가 가까이 되는 것 같아서.... 받아야 아래 코드가 정상적으로 작동하더랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Introduction\n",
    "- (자연어 처리는 여전히 어려운 주제이지만....) \"자연어도 결국 numeric prediction이며, 단지 자연어를 어떻게 numerical하게 표현할 것인가의 단계가 사전에 필요할 뿐이다.\"는 이야기를 들은 뒤부터 문제 접근이 조금 명확해졌습니다.\n",
    "    - 1) 자연어의 numeric한 표현\n",
    "    - 2) numeric data를 이용한 분석\n",
    "    - 3) numeric data를 자연어로 다시 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 자연어의 numeric한 표현\n",
    "- 우선 자연어를 어떤 단위로 분석할지 결정, 즉 token을 어떤 걸로 할 것인가. : <br> token은 형태소/단어/n-gram/문장/문단/문서 등이 될 수 있으며 목적에 따라 적절한 것을 고른다. 다만, 일반적으로 더 작은 단위로 쪼개질수록 이후 비용이 증가한다.\n",
    "- token이 정해지면 이를 어떤 형태의 수치로 나타낼지 결정 : <br> 각 token을 one-hot을 이용한 binary value로 나타낼지/count를 기준으로 한 frequency를 사용할지/tf-idf 값을 이용할지/word embedding을 통해 n차원의 실수 좌표값으로 만들 것인지. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) numeric data를 이용한 분석 \n",
    "- 이후 분석은 일반적인 데이터 분석 프로세스와 동일하다. 유사도/예측 등 numeric data로 할 수 있는 분석은 모두 가능하다. 하지만, 주의해야할 점은 단순히 one-hot이나 frequency, tf-idf로 수치화된 데이터는 각 token 간의 관계가 반영되지 않고, 의미를 가지지 않은 관측치기 때문에 큰 문제가 없다. 하지만 word embedding의 경우, token 간의 semantic relationship을 고려하는 방법이기 때문에 해당 변환된 data가 자연어 간의 관계를 잘 반영하도록 변환되었는지 확인할 필요가 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3) numeric data를 자연어로 다시 변환\n",
    "- 단어의 인덱스를 이용해 변환하거나(idx2word, word2idx 등의 사전 형태를 이용하는 등), word embedding된 경우 가장 가까운 좌표의 단어를 반환하거나 하는 방식으로 본래의 자연어로 변환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Tokenize\n",
    "- 주어진 자연어(텍스트)를 특정 기준에 따라 나누는 것. 이 때, 특정 기준을 token이라고 하며, 이 작업을 tokenize라고 한다. \n",
    "- token은 문단/문장/단어/형태소 등이 될 수 있으며, token은 이 후 분석에 의미를 갖는 최소 단위가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input text\n",
    "input_text = \"Do you know how tokenization works? It's actually quite interesting! Let's analyze a couple of senteces and figure it out.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenizer:\n",
      "['Do you know how tokenization works?', \"It's actually quite interesting!\", \"Let's analyze a couple of senteces and figure it out.\"]\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "print(\"\\nSentence tokenizer:\")\n",
    "print(sent_tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!', 'Let', \"'s\", 'analyze', 'a', 'couple', 'of', 'senteces', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "#word tokenization\n",
    "print(\"\\nWord tokenizer:\")\n",
    "print(word_tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word punct tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'\", 's', 'actually', 'quite', 'interesting', '!', 'Let', \"'\", 's', 'analyze', 'a', 'couple', 'of', 'senteces', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "#WordPunct tokenization\n",
    "print(\"\\nWord punct tokenizer:\")\n",
    "print(WordPunctTokenizer().tokenize(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br><hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2) Stemming\n",
    "- stemming : 주어진 단어의 형태만을 고려한 어간추출 \n",
    "- lemmatizing : 주어진 단어의 문맥 정보까지 고려한 표제화 (lemma : 단어의 원형)\n",
    "- 예를 들어, 영어로 \"flies\"라는 단어가 주어졌을 때 stemming은 주어진 \"files\"에서 일반적으로 복수나 3인칭 단수 형태에 붙는 \"-es\"를 제거한 어간인 \"fli\"를 반환하지만, lemmatizing의 경우 문장 내에서 \"flies\"가 \"날다(동사)\"의 3인칭 단수형인지 \"파리(명사)\"의 복수형인지를 구분하고 \"fly\"를 반환한다는 차이가 있다.\n",
    "    * 앞선 \"fli\"는 \"-es\"만 뗀 형태인데 반해, \"fly\"는 기본 사전형을 알고 있어야 반환 가능한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_words = [\"writing\", \"calves\", \"be\", \"branded\", \"horse\", \"randomize\", \"possibly\", \n",
    "               \"provision\", \"hospital\", \"kept\", \"scratchy\", \"code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stemmer engine \n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL\n",
      "====================================================================\n",
      "         writing           write            writ           write\n",
      "          calves            calv            calv            calv\n",
      "              be              be              be              be\n",
      "         branded           brand           brand           brand\n",
      "           horse            hors            hors            hors\n",
      "       randomize          random          random          random\n",
      "        possibly         possibl            poss         possibl\n",
      "       provision          provis          provid          provis\n",
      "        hospital          hospit          hospit          hospit\n",
      "            kept            kept            kept            kept\n",
      "        scratchy        scratchi        scratchy        scratchi\n",
      "            code            code             cod            code\n"
     ]
    }
   ],
   "source": [
    "stemmer_names = [\"PORTER\", \"LANCASTER\", \"SNOWBALL\"]\n",
    "formatted_text = \"{:>16}\" * (len(stemmer_names) + 1)\n",
    "print(\"\\n\", formatted_text.format(\"INPUT WORD\", *stemmer_names))\n",
    "print(\"=\"*68)\n",
    "\n",
    "for word in input_words : \n",
    "    output = [word, porter.stem(word), lancaster.stem(word), snowball.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어 형태소 분석기별 성능 비교\n",
    "- 압축 정도 (압축 정도가 높을수록 글자수를 많이 줄이고 본래 형태를 알아보기 어렵지만, 더 많은 형태의 단어를 동일 어간으로 분류 가능하다.) : <br> lancaster > snowball > porter\n",
    "- 속도 : <br> lancaster > snowball > porter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_words = [\"writing\", \"calves\", \"be\", \"branded\", \"horse\", \"randomize\", \"possibly\", \n",
    "               \"provision\", \"hospital\", \"kept\", \"scratchy\", \"code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating lemmatizing object \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER\n",
      "===========================================================================\n",
      "                 writing                 writing                   write\n",
      "                  calves                    calf                   calve\n",
      "                      be                      be                      be\n",
      "                 branded                 branded                   brand\n",
      "                   horse                   horse                   horse\n",
      "               randomize               randomize               randomize\n",
      "                possibly                possibly                possibly\n",
      "               provision               provision               provision\n",
      "                hospital                hospital                hospital\n",
      "                    kept                    kept                    keep\n",
      "                scratchy                scratchy                scratchy\n",
      "                    code                    code                    code\n"
     ]
    }
   ],
   "source": [
    "lemmatizer_names = [\"NOUN LEMMATIZER\", \"VERB LEMMATIZER\"]\n",
    "formatted_text = \"{:>24}\" * (len(lemmatizer_names)+1)\n",
    "print(\"\\n\", formatted_text.format(\"INPUT WORD\", *lemmatizer_names))\n",
    "print(\"=\"*75)\n",
    "\n",
    "for word in input_words : \n",
    "    output = [word, lemmatizer.lemmatize(word, pos=\"n\"), lemmatizer.lemmatize(word, pos=\"v\")]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Chunking \n",
    "- 단순히 문장을 (\" \"을 기준으로) 단어로 쪼개서 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#입력텍스트를 단어 묶음으로 나눈다. \n",
    "#단어 묶음별로 N개의 단어를 포함한다.\n",
    "def chunker(input_data, N) : \n",
    "    input_words = input_data.split(\" \")\n",
    "    output = []\n",
    "    cur_chunk = []\n",
    "    count = 0\n",
    "    \n",
    "    for word in input_words : \n",
    "        cur_chunk.append(word) \n",
    "        count += 1\n",
    "        if count == N : \n",
    "            output.append(\" \".join(cur_chunk))\n",
    "            count, cur_chunk = 0, []\n",
    "    output.append(\" \".join(cur_chunk))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = \" \".join(brown.words()[:12000])\n",
    "chunk_size = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text chunks = 18 \n",
      "\n",
      "Chunk 1 ==> The Fulton County Grand Jury said Friday an invest\n",
      "Chunk 2 ==> '' . ( 2 ) Fulton legislators `` work with city of\n",
      "Chunk 3 ==> . Construction bonds Meanwhile , it was learned th\n",
      "Chunk 4 ==> , anonymous midnight phone calls and veiled threat\n",
      "Chunk 5 ==> Harris , Bexar , Tarrant and El Paso would be $451\n",
      "Chunk 6 ==> set it for public hearing on Feb. 22 . The proposa\n",
      "Chunk 7 ==> College . He has served as a border patrolman and \n",
      "Chunk 8 ==> of his staff were doing on the address involved co\n",
      "Chunk 9 ==> plan alone would boost the base to $5,000 a year a\n",
      "Chunk 10 ==> nursing homes In the area of `` community health s\n",
      "Chunk 11 ==> of its Angola policy prove harsh , there has been \n",
      "Chunk 12 ==> system which will prevent Laos from being used as \n",
      "Chunk 13 ==> reform in recipient nations . In Laos , the admini\n",
      "Chunk 14 ==> . He is not interested in being named a full-time \n",
      "Chunk 15 ==> said , `` to obtain the views of the general publi\n",
      "Chunk 16 ==> '' . Mr. Reama , far from really being retired , i\n",
      "Chunk 17 ==> making enforcement of minor offenses more effectiv\n",
      "Chunk 18 ==> to tell the people where he stands on the tax issu\n"
     ]
    }
   ],
   "source": [
    "chunks = chunker(input_data, chunk_size) \n",
    "print(\"\\nNumber of text chunks =\", len(chunks), \"\\n\")\n",
    "for i, chunk in enumerate(chunks) : \n",
    "    print(\"Chunk\", i+1, \"==>\", chunk[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Bag of Word\n",
    "- 텍스트 데이터를 분석에 이용하기 위해선 수치화가 필요하다. Bag of Word는 문서에서 사용된 모든 단어가 포함된 어휘 목록을 만들고, 이 어휘 목록을 열(차원)으로 가지고 문장(또는 문단/문서)의 index를 행으로 가지는 frequency table를 만든다. 이 frequency table을 \"document term matrix\"라고 한다. \n",
    "- 이 작업을 통해 각 문장/문단/문서는 \"단어의 집합\"으로 표현된다. 대신 행이 많아질수록, 행렬이 희소해지고 열이 많아진다. 일반적으로 document term matrix를 희소 행렬이라고 하는 이유.\n",
    "- 분석 용도에 따라 frequency 값 대신 tf-idf 등의 값을 넣어서 사용하기도 한다. 그럴 땐 아래에선 sklearn의 CountVectorizer를 썼는데, TfidfVectorizer를 사용하면 된다.\n",
    "    - eg. TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data from \"brown\" corpus\n",
    "input_data = \" \".join(brown.words()[:5400])\n",
    "chunk_size = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunks = []\n",
    "text_chunks = chunker(input_data, chunk_size)\n",
    "for count, chunk in enumerate(text_chunks) : \n",
    "    d = {\"index\":count, \"text\":chunk}\n",
    "    chunks.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#document term matrix\n",
    "count_vectorizer = CountVectorizer(min_df=7, max_df=20) #CountVectorizer 작업 수행할 객체 생성. 앞선 tagging 객체 생성한 것과 같은 맥락\n",
    "document_term_matrix = count_vectorizer.fit_transform([chunk['text'] for chunk in chunks]) #그 객체를 이용해 data 처리하고, 객체에 처리 정보를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      " ['and' 'are' 'be' 'by' 'county' 'for' 'in' 'is' 'it' 'of' 'on' 'one' 'said'\n",
      " 'state' 'that' 'the' 'to' 'two' 'was' 'which' 'with']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = np.array(count_vectorizer.get_feature_names()) # get_feature_names() 방금 그 객체에 저장된 dtm의 col names 반환\n",
    "print(\"\\nVocabulary:\\n\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#naming by chunks\n",
    "chunk_names = []\n",
    "for i in range(len(text_chunks)) : \n",
    "    chunk_names.append(\"Chunk-\"+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document term matrix:\n",
      "\n",
      "         Word     Chunk-1     Chunk-2     Chunk-3     Chunk-4     Chunk-5     Chunk-6     Chunk-7 \n",
      "\n",
      "         and          23           9           9          11           9          17          10\n",
      "         are           2           2           1           1           2           2           1\n",
      "          be           6           8           7           7           6           2           1\n",
      "          by           3           4           4           5          14           3           6\n",
      "      county           6           2           7           3           1           2           2\n",
      "         for           7          13           4          10           7           6           4\n",
      "          in          15          11          15          11          13          14          17\n",
      "          is           2           7           3           4           5           5           2\n",
      "          it           8           6           8           9           3           1           2\n",
      "          of          31          20          20          30          29          35          26\n",
      "          on           4           3           5          10           6           5           2\n",
      "         one           1           3           1           2           2           1           1\n",
      "        said          12           5           7           7           4           3           7\n",
      "       state           3           7           2           6           3           4           1\n",
      "        that          13           8           9           2           7           1           7\n",
      "         the          71          51          43          51          43          52          49\n",
      "          to          11          26          20          26          21          15          11\n",
      "         two           2           1           1           1           1           2           2\n",
      "         was           5           6           7           7           4           7           3\n",
      "       which           7           4           5           4           3           1           1\n",
      "        with           2           2           3           1           2           2           3\n"
     ]
    }
   ],
   "source": [
    "#print dtm\n",
    "print(\"\\nDocument term matrix:\")\n",
    "formatted_text = \"{:>12}\" * (len(chunk_names)+1)\n",
    "print(\"\\n\", formatted_text.format(\"Word\", *chunk_names), \"\\n\")\n",
    "\n",
    "for word, item in zip(vocabulary, document_term_matrix.T) : \n",
    "    output = [word] + [str(freq) for freq in item.data] #formatting에 쓰기 위해 str()로 변환\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 번외) 한국어 형태소 분석기\n",
    "(이 부분은 도메인 지식이 부족해서 다소 주관적인 느낌에 근거한 내용)\n",
    "- 한국어 형태소 분석기에선 명사 표제화, 동사 표제화나 어간 추출이 분리되어 있는 것 같진 않습니다. 대부분의 단어로 나누는 작업은 아래 형태소 분석기를 사용하고 품사를 고려해서 나누는 걸 보면 표제화에 가까운 것 같은데, 명사 표제화/동사 표제화로 나뉘진 않는 듯.\n",
    "- 추가로 단어는 품사에 따라 뜻이 달라지는데 그 정보를 고려하지 않은 단어의 기본형은 본래 단어가 지니는 정보를 희석시키고 있기 때문에, 분석단에서 stemming보단 lemmatizing을 사용하는 게 좋다고 생각."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kkma = Kkma()\n",
    "twitter = Twitter()\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_text = \"우원식 더불어민주당 원내대표가 31일 “여야와 모든 경제주체가 참여하는 가칭 ‘사회적 연대위원회’를 국회 내에 구성하자”고 제안했다. \\\n",
    "              이를 통해 “불평등과 양극화, 불안과 고통으로 가득 찬 전쟁터와 같은” 국민들의 현실을 근본적으로 바꿔내야 한다는 것이다. \\\n",
    "              아울러 서민의 삶과 직결된 부동산시장 안정을 위해 “보유세 인상과 분양원가 공개를 포함해 특단의 대책을 강구하겠다”고 밝혔다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sentence = input_text.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morp_engine = [\"Kkma\", \"Twitter\", \"Mecab\"]\n",
    "kkma_pos = kkma.pos(input_sentence[1])\n",
    "twit_pos = twitter.pos(input_sentence[1])\n",
    "mecab_pos = mecab.pos(input_sentence[1])\n",
    "pos_result = [kkma_pos, twit_pos, mecab_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Kkma  :  이르/ ㄹ/통해/ “/불평등/ 과/양극화/ ,/불안/ 과/고통/으로/가득/ 차/ ㄴ/전쟁터/ 와/ 같/ 은/ ”/국민/ 들/ 의/현실/ 을/근본적/으로/바꾸/ 어/ 내/아야/ 하/ㄴ다는/ 것/ 이/ 다/\n",
      "   Twitter  :  이를/통해/ “/불평등/ 과/양극화/ ,/불안/ 과/고통/으로/가득/ 찬/전쟁/ 터/ 와/같은/ ”/국민/ 들/ 의/현실/ 을/근본/ 적/으로/바꿔/ 내/ 야/ 한/다는/ 것/이다/\n",
      "     Mecab  :   이/ 를/통해/ “/ 불/평등/ 과/양극/ 화/ ,/불안/ 과/고통/으로/가득/ 찬/전쟁터/ 와/ 같/ 은/ ”/국민/ 들/ 의/현실/ 을/근본/ 적/으로/바꿔/ 내/ 야/한다는/ 것/ 이/ 다/\n"
     ]
    }
   ],
   "source": [
    "for name, result in zip(morp_engine, pos_result) : \n",
    "    formatted_text = \"{:>10}\"+\"{:>2}\"+\"{:>2}/\" * (len(result))\n",
    "    formatted_text2 = \"{:>10}\"+\"{:>2}\"+\"{:>2}/\" * (len(result))\n",
    "    stem = [x[0] for x in result]\n",
    "    tag = [x[1] for x in result]\n",
    "    print(formatted_text.format(name,\"  :  \",*stem))\n",
    "    #print(formatted_text2.format(\"\",\"  :  \",*tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Kkma             Twitter               Mecab\n",
      "====================================================================\n",
      "        ('이르', 'VV')      ('이를', 'Verb')         ('이', 'NP')\n",
      "        ('ㄹ', 'ETD')      ('통해', 'Noun')        ('를', 'JKO')\n",
      "       ('통해', 'NNG')    ('“', 'Foreign')     ('통해', 'VV+EC')\n",
      "         ('“', 'SS')     ('불평등', 'Noun')        ('“', 'SSO')\n",
      "      ('불평등', 'NNG')       ('과', 'Josa')        ('불', 'XPN')\n",
      "        ('과', 'JKM')     ('양극화', 'Noun')       ('평등', 'NNG')\n",
      "      ('양극화', 'NNG')(',', 'Punctuation')         ('과', 'JC')\n",
      "         (',', 'SP')      ('불안', 'Noun')       ('양극', 'NNG')\n",
      "       ('불안', 'NNG')       ('과', 'Josa')        ('화', 'XSN')\n",
      "         ('과', 'JC')      ('고통', 'Noun')         (',', 'SC')\n",
      "       ('고통', 'NNG')      ('으로', 'Josa')       ('불안', 'NNG')\n",
      "       ('으로', 'JKM')      ('가득', 'Noun')         ('과', 'JC')\n",
      "       ('가득', 'MAG')       ('찬', 'Noun')       ('고통', 'NNG')\n",
      "         ('차', 'VV')      ('전쟁', 'Noun')       ('으로', 'JKB')\n",
      "        ('ㄴ', 'ETD')       ('터', 'Noun')       ('가득', 'MAG')\n",
      "      ('전쟁터', 'NNG')       ('와', 'Josa')     ('찬', 'VV+ETM')\n",
      "        ('와', 'JKM') ('같은', 'Adjective')      ('전쟁터', 'NNG')\n",
      "         ('같', 'VA')    ('”', 'Foreign')        ('와', 'JKB')\n",
      "        ('은', 'ETD')      ('국민', 'Noun')         ('같', 'VA')\n",
      "         ('”', 'SS')     ('들', 'Suffix')        ('은', 'ETM')\n",
      "       ('국민', 'NNG')       ('의', 'Josa')        ('”', 'SSC')\n",
      "        ('들', 'XSN')      ('현실', 'Noun')       ('국민', 'NNG')\n",
      "        ('의', 'JKG')       ('을', 'Josa')        ('들', 'XSN')\n",
      "       ('현실', 'NNG')      ('근본', 'Noun')        ('의', 'JKG')\n",
      "        ('을', 'JKO')     ('적', 'Suffix')       ('현실', 'NNG')\n",
      "      ('근본적', 'NNG')      ('으로', 'Josa')        ('을', 'JKO')\n",
      "       ('으로', 'JKM')      ('바꿔', 'Verb')       ('근본', 'NNG')\n",
      "        ('바꾸', 'VV')    ('내', 'PreEomi')        ('적', 'XSN')\n",
      "        ('어', 'ECS')       ('야', 'Eomi')       ('으로', 'JKB')\n",
      "        ('내', 'VXV')       ('한', 'Verb')     ('바꿔', 'VV+EC')\n",
      "       ('아야', 'ECD')      ('다는', 'Eomi')         ('내', 'VX')\n",
      "         ('하', 'VV')       ('것', 'Noun')         ('야', 'EC')\n",
      "      ('ㄴ다는', 'ETD')      ('이다', 'Josa')   ('한다는', 'VX+ETM')\n",
      "        ('것', 'NNB')                 (,)        ('것', 'NNB')\n",
      "        ('이', 'VCP')                 (,)        ('이', 'VCP')\n",
      "        ('다', 'EFN')                 (,)         ('다', 'EC')\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(kkma_pos), len(twit_pos), len(mecab_pos))\n",
    "temp = [result+[\"(,)\"]*(max_len-len(result)) for result in pos_result]\n",
    "temp = [[str(x) for x in result] for result in temp]\n",
    "\n",
    "formatted_text3 = \"{:>20}\" * (len(morp_engine))\n",
    "print(\"\\n\", formatted_text3.format(*morp_engine))\n",
    "print(\"=\"*68)\n",
    "for word in zip(*temp) : \n",
    "    print(formatted_text3.format(*word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 형태소 분석기별 성능 비교\n",
    "- 속도 : <br> Mecab > Twitter > Kkma\n",
    "- tagging 성능 (지극히 주관적) : <br> Mecab > Twitter = Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
