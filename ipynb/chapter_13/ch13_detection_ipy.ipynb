{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13 물체 감지와 추적\n",
    "\n",
    "- Opencv 설치\n",
    "- 프레임 차이 계산법\n",
    "- 색 공간을 이용한 물체 추적 기법\n",
    "- 배경 분리 기법을 이용한 물체 추적 기법\n",
    "- 캠시프트 알고리즘으로 인터렉티브 방식 물체 추적기를 만드는 방법\n",
    "- 광학흐름 기반 추적 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. opencv 설치 가이드\n",
    "https://www.learnopencv.com/install-opencv3-on-macos/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/usr/local/lib/python3.6/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 프레임 차이 대조법\n",
    "(https://www.youtube.com/watch?v=DrCsvqE18u0)\n",
    "\n",
    "- 실시간 비디오 스트림에서 캡처한 연속된 프레임들의 차이점을 분석 >> 움직이는 부분을 탐지하는 기법\n",
    "- 노이즈에 굉장히 민감 >> 물체를 정확히 추적하기 힘듦\n",
    "\n",
    "#### 주요 함수\n",
    "- frame_diff( prev_frame, cur_frame, next_frame )\n",
    "    - <b>cv2.absdiff</b> : Calculates the per-element absolute difference between two arrays or between an array and a scalar.\n",
    "    - <b>cv2.bitwise</b> : Calculates the per-element bit-wise conjunction of two arrays or an array and a scalar.\n",
    "\n",
    "- get_frame( cap, scaling_factor )\n",
    "    - <b>resize</b> : Resizes an image.\n",
    "    - <b>cvtColor</b>\n",
    "\n",
    "\n",
    "출처 : https://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Compute the frame differences\n",
    "def frame_diff(prev_frame, cur_frame, next_frame):\n",
    "    # 현재 프레임과 다음 프레임의 엘리먼트별 절대값 차이 구하기\n",
    "    diff_frames_1 = cv2.absdiff(next_frame, cur_frame)\n",
    "\n",
    "    # 현재 프레임과 이전 프레임의 엘리먼트별 절대값 차이 구하기\n",
    "    diff_frames_2 = cv2.absdiff(cur_frame, prev_frame)\n",
    "\n",
    "    return cv2.bitwise_and(diff_frames_1, diff_frames_2) ## 두 차이점을 비트단위로 AND 연산을 수행 후 결과 반환\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to get the current frame from the webcam\n",
    "def get_frame(cap, scaling_factor):\n",
    "    # 현재 비디오에서 프레임 읽기\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    # 이미지 크기 조정\n",
    "    # https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html\n",
    "    frame = cv2.resize(frame, None, fx=scaling_factor, \n",
    "            fy=scaling_factor, interpolation=cv2.INTER_AREA) \n",
    "    ## fx - horizontal, fy = vertical, \n",
    "    ## interpolation(보간법) : INTER_AREA - resampling using pixel area relation\n",
    "    \n",
    "\n",
    "    # 흑백으로 전환\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    return gray \n",
    "\n",
    "if __name__=='__main__':\n",
    "    # 비디오 캡쳐 오브젝트 초기화\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # 이미지 비율 정의 ( 출력 화면 크기가 변한다. 1.5쯤 하면 맥 13인치 전체 크기)\n",
    "    scaling_factor = 1.0\n",
    "    \n",
    "    ## 첫번째 프레임 변수 초기화\n",
    "    # Grab the current frame (리스트 형태로 프레임을 저장하게 된다.)\n",
    "    prev_frame = get_frame(cap, scaling_factor) \n",
    "    ##print(prev_frame)\n",
    "    # Grab the next frame\n",
    "    cur_frame = get_frame(cap, scaling_factor) \n",
    "    # Grab the frame after that\n",
    "    next_frame = get_frame(cap, scaling_factor) \n",
    "\n",
    "    # Keep reading the frames from the webcam \n",
    "    # until the user hits the 'Esc' key\n",
    "    while True:\n",
    "        # Display the frame difference\n",
    "        cv2.imshow('Object Movement', frame_diff(prev_frame, \n",
    "                cur_frame, next_frame))\n",
    "\n",
    "        # Update the variables\n",
    "        prev_frame = cur_frame\n",
    "        cur_frame = next_frame \n",
    "\n",
    "        # Grab the next frame\n",
    "        next_frame = get_frame(cap, scaling_factor)\n",
    "\n",
    "        # Check if the user hit the 'Esc' key\n",
    "        key = cv2.waitKey(10)\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "    # Close all the windows\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 색 공간을 이용한 물체 추적 기법\n",
    "https://www.youtube.com/watch?v=hWcxEkWLm9o\n",
    "https://www.youtube.com/watch?v=9qky6g8NRmI\n",
    "\n",
    "\n",
    "- RGB > HSV로 변환\n",
    "- 추적할 물체의 색상에 대한 임계값(threshold)를 이용하여 물체를 추적\n",
    "\n",
    "- <b>함수</b> 그리고 <b>functions</b>    \n",
    "    - cv2.inRange\n",
    "    - median_blurs : Blurs an image using the median filter.\n",
    "    - cv2.imshow : show images\n",
    "    - cv2.waitkey : delay – Delay in milliseconds. 0 is the special value that means “forever”.\n",
    "    \n",
    "https://docs.opencv.org/2.4/modules/highgui/doc/user_interface.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def get_frame(cap, scaling_factor):\n",
    "    # 현재 비디오에서 프레임 읽기\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    # 이미지 크기 조정\n",
    "    # https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html\n",
    "    frame = cv2.resize(frame, None, fx=scaling_factor, \n",
    "            fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return frame\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # 초기화\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # 출력창 크기 설정\n",
    "    scaling_factor = 0.5\n",
    "\n",
    "    # Keep reading the frames from the webcam \n",
    "    # until the user hits the 'Esc' key\n",
    "    while True:\n",
    "        # 프레임 가져오기\n",
    "        frame = get_frame(cap, scaling_factor) \n",
    "\n",
    "        # hsv 컬러로 변환\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # 사람 피부색에 가까운 HSV 값을 설정\n",
    "        lower = np.array([0, 70, 60])\n",
    "        upper = np.array([50, 150, 255])\n",
    "\n",
    "        # hsv 컬러상에서 lower~upper사이의 element들을 선별\n",
    "        mask = cv2.inRange(hsv, lower, upper)\n",
    "\n",
    "        # bit 연산을 통해 원본에서 선별된 element들을 출력\n",
    "        img_bitwise_and = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "        # Run median blurring\n",
    "        # Blurs an image using the median filter.\n",
    "        # http://terms.naver.com/entry.nhn?docId=2762767&cid=50307&categoryId=50307\n",
    "        img_median_blurred = cv2.medianBlur(img_bitwise_and, 5)\n",
    "\n",
    "        # Display the input and output\n",
    "        cv2.imshow('Input', frame)\n",
    "        cv2.imshow('Output', img_median_blurred)\n",
    "\n",
    "        # Check if the user hit the 'Esc' key\n",
    "        c = cv2.waitKey(5) \n",
    "        if c == 27:\n",
    "            break\n",
    "\n",
    "    # Close all the windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 배경분리법\n",
    "https://www.youtube.com/watch?v=KRKKektCcok\n",
    "\n",
    "https://docs.opencv.org/3.3.0/db/d5c/tutorial_py_bg_subtraction.html\n",
    "\n",
    "- 주어진 영상에서 배경에 해당하는 모델을 만들고, 이를 이용해 움직이는 물체를 추출하는 기법\n",
    "- 정적인 배경에서 움직이는 물체를 감지하는 데 뛰어난 성능을 발휘\n",
    "- 배경모델을 만든 후 이 모델이 표현하는 배경을 현재 프레임에서 제거해 전경(foreground)만을 추출\n",
    "- 배경모델을 실시간으로 업데이트 한다는 점에서 프레임 차이법과의 차이점 > 기준점이 계속 변하는 적응형 알고리즘으로 구현\n",
    "\n",
    "- 함수 & 객체\n",
    "    - 배경제거 관련 객체 : createBackgroundSubtractorMOG2\n",
    "    - bg_subtractor.apply( frame, learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to get the current frame from the webcam\n",
    "def get_frame(cap, scaling_factor):\n",
    "    # Read the current frame from the video capture object\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    # Resize the image\n",
    "    frame = cv2.resize(frame, None, fx=scaling_factor, \n",
    "            fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return frame\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Define the video capture object\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Define the background subtractor object\n",
    "    bg_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "  \n",
    "    ## 학습에 사용할 이전 프레임의 갯수\n",
    "    ## 이를 통해 알고리즘 학습속도 제어\n",
    "    ## 히스토리값이 클수록 학습 속도가 느려짐 \n",
    "    history = 100\n",
    "\n",
    "    # Define the learning rate\n",
    "    learning_rate = 1.0/history\n",
    "\n",
    "    # Keep reading the frames from the webcam \n",
    "    # until the user hits the 'Esc' key\n",
    "    while True:\n",
    "        # Grab the current frame\n",
    "        frame = get_frame(cap, 0.5)\n",
    "\n",
    "        # \n",
    "        mask = bg_subtractor.apply(frame, learningRate=learning_rate)\n",
    "\n",
    "        # Convert grayscale image to RGB color image\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Display the images\n",
    "        cv2.imshow('Input', frame)\n",
    "        cv2.imshow('Output', mask & frame) ## &을 통해 비트 연산 가능\n",
    "\n",
    "        # Check if the user hit the 'Esc' key\n",
    "        c = cv2.waitKey(10)\n",
    "        if c == 27:\n",
    "            break\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    # Close all the windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 캠시프트 알고리즘을 통한 인터렉티브 방식 물체 추적기\n",
    "https://www.youtube.com/watch?v=iBOlbs8i7Og\n",
    "\n",
    "- 색 공간 기반 추적기법은 색상을 임의로 지정해주어야 한다는 한계가 존재\n",
    "- mean shift를 개선한 버전의 알고리즘인 캠시프트\n",
    "- 관심영역을 설정 : 추적할 물체 주변에 경계선을 그은 것\n",
    "    - 해당 영역에 대한 컬러 히스토 그램을 기준으로 점의 집합을 선택 및 점들의 중심 구하기\n",
    "    - 점들의 중심이 관심영역의 기하 중심에있다면 물체는 이동하지 않았다고 판단\n",
    "    - 점들의 중심점이 관심영역의 중심과 다르다면 물체가 이동했다고 판단 > 경계선을 이동\n",
    "    \n",
    "- 함수\n",
    "    - cv2.VideoCapture(0)\n",
    "    - read()\n",
    "    - cv2.namedWindow('Object Tracker')\n",
    "    - cv2.setMouseCallback('Object Tracker', self.mouse_event)\n",
    "    -\n",
    "\n",
    "\n",
    "https://docs.opencv.org/3.4.0/db/df8/tutorial_py_meanshift.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define a class to handle object tracking related functionality\n",
    "class ObjectTracker(object):\n",
    "    def __init__(self, scaling_factor=0.5):\n",
    "        # 초기화\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "\n",
    "        # 현재 프레임 캡쳐\n",
    "        _, self.frame = self.cap.read()\n",
    "\n",
    "        # 출력창 크기 조절 인자\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "        # Resize the frame\n",
    "        self.frame = cv2.resize(self.frame, None, \n",
    "                fx=self.scaling_factor, fy=self.scaling_factor, \n",
    "                interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # 프레임 표시할 창 띄우기\n",
    "        cv2.namedWindow('Object Tracker')\n",
    "\n",
    "        # 마우스로 입력받기 위한 마우스 콜백함수 설정\n",
    "        cv2.setMouseCallback('Object Tracker', self.mouse_event)\n",
    "\n",
    "        # 직사각형 모양의 선택 영역에 대한 변수 초기화\n",
    "        self.selection = None\n",
    "        # 시작 위치에 대한 변수 초기화\n",
    "        self.drag_start = None\n",
    "        # 추적 상태에 대한 변수 초기화\n",
    "        self.tracking_state = 0\n",
    "\n",
    "    # 마우스 이벤트 추적용 메소드\n",
    "    def mouse_event(self, event, x, y, flags, param):\n",
    "        ## x,y 좌표를 16비트 numpy 정수로 변환\n",
    "        x, y = np.int16([x, y]) \n",
    "        ##print(x, y)\n",
    "\n",
    "        ## 마우스 클릭 발생시 실행\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            self.drag_start = (x, y)\n",
    "            self.tracking_state = 0\n",
    "\n",
    "        ## 드래그 실행시 & 영역 설정 종료시 \n",
    "        if self.drag_start:\n",
    "            if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "                # Extract the dimensions of the frame\n",
    "                h, w = self.frame.shape[:2]\n",
    "\n",
    "                # Get the initial position\n",
    "                xi, yi = self.drag_start\n",
    "\n",
    "                # Get the max and min values\n",
    "                x0, y0 = np.maximum(0, np.minimum([xi, yi], [x, y]))\n",
    "                x1, y1 = np.minimum([w, h], np.maximum([xi, yi], [x, y]))\n",
    "\n",
    "                # Reset the selection variable\n",
    "                self.selection = None\n",
    "\n",
    "                # Finalize the rectangular selection\n",
    "                if x1-x0 > 0 and y1-y0 > 0:\n",
    "                    self.selection = (x0, y0, x1, y1)\n",
    "\n",
    "            else:\n",
    "                # If the selection is done, start tracking  \n",
    "                self.drag_start = None\n",
    "                if self.selection is not None:\n",
    "                    self.tracking_state = 1\n",
    "\n",
    "    # Method to start tracking the object\n",
    "    def start_tracking(self):\n",
    "        # Iterate until the user presses the Esc key\n",
    "        while True:\n",
    "            # Capture the frame from webcam\n",
    "            _, self.frame = self.cap.read()\n",
    "            \n",
    "            # Resize the input frame\n",
    "            self.frame = cv2.resize(self.frame, None, \n",
    "                    fx=self.scaling_factor, fy=self.scaling_factor, \n",
    "                    interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Create a copy of the frame\n",
    "            vis = self.frame.copy()\n",
    "\n",
    "            # Convert the frame to HSV colorspace\n",
    "            hsv = cv2.cvtColor(self.frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "            # Create the mask based on predefined thresholds\n",
    "            mask = cv2.inRange(hsv, np.array((0., 60., 32.)), \n",
    "                        np.array((180., 255., 255.)))\n",
    "\n",
    "            # Check if the user has selected the region\n",
    "            if self.selection:\n",
    "                # Extract the coordinates of the selected rectangle\n",
    "                x0, y0, x1, y1 = self.selection\n",
    "\n",
    "                # Extract the tracking window\n",
    "                self.track_window = (x0, y0, x1-x0, y1-y0)\n",
    "\n",
    "                # Extract the regions of interest \n",
    "                hsv_roi = hsv[y0:y1, x0:x1]\n",
    "                mask_roi = mask[y0:y1, x0:x1]\n",
    "\n",
    "                # Compute the histogram of the region of \n",
    "                # interest in the HSV image using the mask\n",
    "                hist = cv2.calcHist( [hsv_roi], [0], mask_roi, \n",
    "                        [16], [0, 180] )\n",
    "\n",
    "                # Normalize and reshape the histogram\n",
    "                cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX);\n",
    "                self.hist = hist.reshape(-1)\n",
    "\n",
    "                # Extract the region of interest from the frame\n",
    "                vis_roi = vis[y0:y1, x0:x1]\n",
    "\n",
    "                # Compute the image negative (for display only)\n",
    "                cv2.bitwise_not(vis_roi, vis_roi)\n",
    "                vis[mask == 0] = 0\n",
    "\n",
    "            # Check if the system in the \"tracking\" mode\n",
    "            if self.tracking_state == 1:\n",
    "                # Reset the selection variable\n",
    "                self.selection = None\n",
    "                \n",
    "                # Compute the histogram back projection\n",
    "                hsv_backproj = cv2.calcBackProject([hsv], [0], \n",
    "                        self.hist, [0, 180], 1)\n",
    "\n",
    "                # Compute bitwise AND between histogram \n",
    "                # backprojection and the mask\n",
    "                hsv_backproj &= mask\n",
    "\n",
    "                # Define termination criteria for the tracker\n",
    "                term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, \n",
    "                        10, 1)\n",
    "\n",
    "                # Apply CAMShift on 'hsv_backproj'\n",
    "                track_box, self.track_window = cv2.CamShift(hsv_backproj, \n",
    "                        self.track_window, term_crit)\n",
    "\n",
    "                # Draw an ellipse around the object\n",
    "                cv2.ellipse(vis, track_box, (0, 255, 0), 2)\n",
    "\n",
    "            # Show the output live video\n",
    "            cv2.imshow('Object Tracker', vis)\n",
    "\n",
    "            # Stop if the user hits the 'Esc' key\n",
    "            c = cv2.waitKey(5)\n",
    "            if c == 27:\n",
    "                break\n",
    "\n",
    "        # Close all the windows\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t# Start the tracker\n",
    "    ObjectTracker().start_tracking()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 광학 흐름 기반 추적 기법\n",
    "https://www.youtube.com/watch?v=MOaKnCSejXQ\n",
    "\n",
    "- 이미지의 특징점을 이용해서 물체를 추적하는 기법 : 현재 프레임에서 일련의 특징점을 발견하면, 이에 대한 <b>변위벡터(displacement vector)</b>를 계산해서 특징점을 추적 (변위벡터 : 움직이는 입자의 나중 위치벡터와 처음 위치벡터의 차이이다. <네이버 두산 백과사전> )\n",
    "- <b>루카스-카나데(Lucas-Kanade)</b> 기법이 가장 유명 \n",
    "    - 1) 현재 프레임에서 특징점을 추출\n",
    "    - 2) x 3 패치(픽셀집합)을 만들기 (하나의 최소 단위)\n",
    "    - 3) 프레임이 변하면서 이전 프레임의 각 픽셀집합들의 특징과 가장 유사한 픽셀집합의 특징을 다음 프레임에서 찾음\n",
    "    - 4) 유사한 픽셀들 간의 이동을 추적함 (변위 벡터를 계산)\n",
    "\n",
    "\n",
    "- 함수\n",
    "    - cv2.calcOpticalFlowPyrLK : Lucas-kanade 방법을 이용하여 optical flow를 계산 (핵심 함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to track the object\n",
    "def start_tracking():\n",
    "    # Initialize the video capture object\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Define the scaling factor for the frames\n",
    "    scaling_factor = 0.5\n",
    "\n",
    "    # Number of frames to track\n",
    "    num_frames_to_track = 5\n",
    "\n",
    "    # Skipping factor\n",
    "    num_frames_jump = 2\n",
    "\n",
    "    # Initialize variables\n",
    "    tracking_paths = []\n",
    "    frame_index = 0\n",
    "\n",
    "    # Define tracking parameters\n",
    "    tracking_params = dict(winSize  = (11, 11), maxLevel = 2,\n",
    "            criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, \n",
    "                10, 0.03))\n",
    "\n",
    "    # Iterate until the user hits the 'Esc' key\n",
    "    while True:\n",
    "        # Capture the current frame\n",
    "        _, frame = cap.read()\n",
    "\n",
    "        # Resize the frame\n",
    "        frame = cv2.resize(frame, None, fx=scaling_factor, \n",
    "                fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Convert to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Create a copy of the frame\n",
    "        output_img = frame.copy()\n",
    "\n",
    "        if len(tracking_paths) > 0:\n",
    "            # Get images\n",
    "            prev_img, current_img = prev_gray, frame_gray\n",
    "\n",
    "            # Organize the feature points\n",
    "            feature_points_0 = np.float32([tp[-1] for tp in \\\n",
    "                    tracking_paths]).reshape(-1, 1, 2)\n",
    "\n",
    "            # Compute optical flow\n",
    "            feature_points_1, _, _ = cv2.calcOpticalFlowPyrLK(\n",
    "                    prev_img, current_img, feature_points_0, \n",
    "                    None, **tracking_params)\n",
    "\n",
    "            # Compute reverse optical flow\n",
    "            feature_points_0_rev, _, _ = cv2.calcOpticalFlowPyrLK(\n",
    "                    current_img, prev_img, feature_points_1, \n",
    "                    None, **tracking_params)\n",
    "\n",
    "            # Compute the difference between forward and \n",
    "            # reverse optical flow\n",
    "            diff_feature_points = abs(feature_points_0 - \\\n",
    "                    feature_points_0_rev).reshape(-1, 2).max(-1)\n",
    "\n",
    "            # Extract the good points\n",
    "            good_points = diff_feature_points < 1\n",
    "\n",
    "            # Initialize variable\n",
    "            new_tracking_paths = []\n",
    "\n",
    "            # Iterate through all the good feature points \n",
    "            for tp, (x, y), good_points_flag in zip(tracking_paths, \n",
    "                        feature_points_1.reshape(-1, 2), good_points):\n",
    "                # If the flag is not true, then continue\n",
    "                if not good_points_flag:\n",
    "                    continue\n",
    "\n",
    "                # Append the X and Y coordinates and check if\n",
    "                # its length greater than the threshold\n",
    "                tp.append((x, y))\n",
    "                if len(tp) > num_frames_to_track:\n",
    "                    del tp[0]\n",
    "\n",
    "                new_tracking_paths.append(tp)\n",
    "\n",
    "                # Draw a circle around the feature points\n",
    "                cv2.circle(output_img, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "            # Update the tracking paths\n",
    "            tracking_paths = new_tracking_paths\n",
    "\n",
    "            # Draw lines\n",
    "            cv2.polylines(output_img, [np.int32(tp) for tp in \\\n",
    "                    tracking_paths], False, (0, 150, 0))\n",
    "\n",
    "        # Go into this 'if' condition after skipping the \n",
    "        # right number of frames\n",
    "        if not frame_index % num_frames_jump:\n",
    "            # Create a mask and draw the circles\n",
    "            mask = np.zeros_like(frame_gray)\n",
    "            mask[:] = 255\n",
    "            for x, y in [np.int32(tp[-1]) for tp in tracking_paths]:\n",
    "                cv2.circle(mask, (x, y), 6, 0, -1)\n",
    "\n",
    "            # Compute good features to track\n",
    "            feature_points = cv2.goodFeaturesToTrack(frame_gray, \n",
    "                    mask = mask, maxCorners = 500, qualityLevel = 0.3, \n",
    "                    minDistance = 7, blockSize = 7) \n",
    "\n",
    "            # Check if feature points exist. If so, append them\n",
    "            # to the tracking paths\n",
    "            if feature_points is not None:\n",
    "                for x, y in np.float32(feature_points).reshape(-1, 2):\n",
    "                    tracking_paths.append([(x, y)])\n",
    "\n",
    "        # Update variables\n",
    "        frame_index += 1\n",
    "        prev_gray = frame_gray\n",
    "\n",
    "        # Display output\n",
    "        cv2.imshow('Optical Flow', output_img)\n",
    "\n",
    "        # Check if the user hit the 'Esc' key\n",
    "        c = cv2.waitKey(1)\n",
    "        if c == 27:\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t# Start the tracker\n",
    "    start_tracking()\n",
    "\n",
    "    # Close all the windows\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < face detection >\n",
    "- os.chdir() 안에 haar_cascade_files가 있는 루트로 변경하고 실행하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "## change directory of root path\n",
    "os.chdir(\"/Users/EunsungJo/Documents/G/datapub/ch_13_detection/haar_cascade_files/\")\n",
    "os.getcwd()\n",
    "\n",
    "# Load the Haar cascade file\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "        'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Check if the cascade file has been loaded correctly\n",
    "if face_cascade.empty():\n",
    "\traise IOError('Unable to load the face cascade classifier xml file')\n",
    "\n",
    "# Initialize the video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the scaling factor\n",
    "scaling_factor = 0.5\n",
    "\n",
    "# Iterate until the user hits the 'Esc' key\n",
    "while True:\n",
    "    # Capture the current frame\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    # Resize the frame\n",
    "    frame = cv2.resize(frame, None, \n",
    "            fx=scaling_factor, fy=scaling_factor, \n",
    "            interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Run the face detector on the grayscale image\n",
    "    face_rects = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Draw a rectangle around the face\n",
    "    for (x,y,w,h) in face_rects:\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 3)\n",
    "\n",
    "    # Display the output\n",
    "    cv2.imshow('Face Detector', frame)\n",
    "\n",
    "    # Check if the user hit the 'Esc' key\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Close all the windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < face detection >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# change directory of root path\n",
    "os.chdir(\"/Users/EunsungJo/Documents/G/datapub/ch_13_detection/haar_cascade_files/\")\n",
    "os.getcwd()\n",
    "\n",
    "# Load the Haar cascade files for face and eye\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "# Check if the face cascade file has been loaded correctly\n",
    "if face_cascade.empty():\n",
    "\traise IOError('Unable to load the face cascade classifier xml file')\n",
    "\n",
    "# Check if the eye cascade file has been loaded correctly\n",
    "if eye_cascade.empty():\n",
    "\traise IOError('Unable to load the eye cascade classifier xml file')\n",
    "\n",
    "# Initialize the video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the scaling factor\n",
    "ds_factor = 0.5\n",
    "\n",
    "# Iterate until the user hits the 'Esc' key\n",
    "while True:\n",
    "    # Capture the current frame\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    # Resize the frame\n",
    "    frame = cv2.resize(frame, None, fx=ds_factor, fy=ds_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Run the face detector on the grayscale image\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # For each face that's detected, run the eye detector\n",
    "    for (x,y,w,h) in faces:\n",
    "        # Extract the grayscale face ROI\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "        # Extract the color face ROI\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Run the eye detector on the grayscale ROI\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        # Draw circles around the eyes\n",
    "        for (x_eye,y_eye,w_eye,h_eye) in eyes:\n",
    "            center = (int(x_eye + 0.5*w_eye), int(y_eye + 0.5*h_eye))\n",
    "            radius = int(0.3 * (w_eye + h_eye))\n",
    "            color = (0, 255, 0)\n",
    "            thickness = 3\n",
    "            cv2.circle(roi_color, center, radius, color, thickness)\n",
    "\n",
    "    # Display the output\n",
    "    cv2.imshow('Eye Detector', frame)\n",
    "\n",
    "    # Check if the user hit the 'Esc' key\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Close all the windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
